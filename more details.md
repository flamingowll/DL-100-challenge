# DL 100 challenge
 2h/d DeepLearning plan.

To sync your 100-day deep learning plan with the **Coursera Deep Learning Specialization by Andrew Ng**, I’ll map the sections from both the plan and the specialization to give you a clear path without altering the structure of your original plan. The specialization has five courses that cover foundational deep learning topics, and we’ll align them with your challenge.

### 1. **Weeks 1–2: Foundations of Deep Learning**
#### **Coursera Course**: [Neural Networks and Deep Learning (Course 1)](https://www.coursera.org/learn/neural-networks-deep-learning)
- **Days 1–7 (Neural Networks Basics)**: Align with Weeks 1-2 of Andrew Ng's first course (Neural Networks and Deep Learning). You’ll learn about perceptrons, gradient descent, and backpropagation in the lectures, which will complement your practice of building neural networks from scratch.
- **Days 8–14 (Deep Learning with TensorFlow/PyTorch)**: In parallel with building the network using TensorFlow/PyTorch, use the final sections of Course 1 on programming a deep neural network and improving the accuracy of models using practical tips.

### 2. **Weeks 3–4: Deep Learning Architectures**
#### **Coursera Course**: [Improving Deep Neural Networks (Course 2)](https://www.coursera.org/learn/deep-neural-network)
- **Days 15–21 (Convolutional Neural Networks)**: Although CNNs are covered in Course 4 (below), use the optimization techniques from **Course 2** (hyperparameter tuning, regularization) to fine-tune your models while implementing CNNs.
- **Days 22–28 (RNNs and LSTMs)**: Course 5 (Sequence Models) covers RNNs and LSTMs in detail. You can start using Course 2 (Improving Neural Networks) to optimize your models as you practice building simple RNNs/LSTMs for text prediction.

### 3. **Weeks 5–6: Deep Learning for Computer Vision**
#### **Coursera Course**: [Convolutional Neural Networks (Course 4)](https://www.coursera.org/learn/convolutional-neural-networks)
- **Days 29–35 (Transfer Learning)**: This aligns well with **Course 4**, which goes into CNNs and transfer learning. The lectures cover key pre-trained models (ResNet, VGG16) and how to apply transfer learning to your custom datasets.
- **Days 36–42 (Data Augmentation and Regularization)**: Use the regularization techniques from **Course 2** and the data augmentation techniques from **Course 4** to enhance your model.

### 4. **Weeks 7–8: NLP with Deep Learning**
#### **Coursera Course**: [Sequence Models (Course 5)](https://www.coursera.org/learn/nlp-sequence-models)
- **Days 43–49 (Word Embeddings and Text Classification)**: Course 5 covers word embeddings (Word2Vec, GloVe) and will sync well with your study of text classification. You'll also work on sentiment analysis and similar tasks in Course 5.
- **Days 50–56 (Transformers and Attention Mechanisms)**: Transformers aren't covered in-depth in Andrew Ng’s specialization, but the RNN and LSTM foundations from **Course 5** will help you understand the basics of sequence modeling. You can supplement this with external resources on transformers and Hugging Face.

### 5. **Weeks 9–10: Advanced Topics in Deep Learning**
#### **Coursera Course**: [Structuring Machine Learning Projects (Course 3)](https://www.coursera.org/learn/machine-learning-projects)
- **Days 57–63 (GANs)**: Although Andrew Ng’s specialization doesn't focus on GANs, the techniques from **Course 3** (structuring ML projects) and **Course 2** (optimizing models) will help you approach GAN training systematically.
- **Days 64–70 (Autoencoders and Dimensionality Reduction)**: These topics aren’t covered explicitly in Andrew Ng’s specialization. However, Course 3 (Structuring Projects) provides useful guidance on how to approach ML projects in general, which you can apply to autoencoders.

### 6. **Weeks 11–12: Specialization and Optimization**
#### **Coursera Course**: [Improving Deep Neural Networks (Course 2)](https://www.coursera.org/learn/deep-neural-network) & [Structuring ML Projects (Course 3)](https://www.coursera.org/learn/machine-learning-projects)
- **Days 71–77 (Hyperparameter Tuning)**: This is directly covered in **Course 2** (Improving Deep Neural Networks) and aligns with your study of hyperparameter tuning, grid search, and random search.
- **Days 78–84 (Model Deployment)**: This is not covered in Andrew Ng’s specialization, so you'll need to use external resources (Flask, TensorFlow Serving) for deploying your models.

### 7. **Weeks 13–14: Capstone Project**
- **Days 85–100 (End-to-End Project)**: Apply the concepts from **Course 3** (Structuring Machine Learning Projects), especially when working on the capstone project. Focus on workflow design, dataset collection, and project structuring as outlined in this course. Use the knowledge from **Course 1–5** to choose the right model (CNN, RNN, Transformer) for your task.

### Summary:
- **Weeks 1–2**: Sync with **Course 1** (Neural Networks and Deep Learning).
- **Weeks 3–4**: Sync with **Courses 2 and 5** (Improving Neural Networks and Sequence Models).
- **Weeks 5–6**: Sync with **Course 4** (Convolutional Neural Networks).
- **Weeks 7–8**: Sync with **Course 5** (Sequence Models).
- **Weeks 9–12**: Use **Courses 2 and 3** for optimization, structuring projects, and deployment.
- **Weeks 13–14**: Capstone project aligned with **Course 3** (Structuring Projects).

This sync provides a more structured and supported learning path by blending your challenge with Andrew Ng’s courses, making it easier to manage both theory and practice.