# DL 100 challenge
 2h/d DeepLearning plan.


### **Weeks 1–2: Foundations of Neural Networks and Deep Learning**  
(Corresponds to Andrew Ng's **Course 1: Neural Networks and Deep Learning**)

#### Days 1–7: Neural Networks Basics
- **Learn:** Perceptrons, activation functions, gradient descent, and backpropagation.
- **Practice:** Build a simple neural network from scratch using Python and NumPy.
- **Project:** Train a small neural network to recognize handwritten digits using the MNIST dataset.

#### Days 8–14: Deep Learning with TensorFlow/PyTorch
- **Learn:** Introduction to TensorFlow or PyTorch (choose one).
- **Practice:** Re-implement your MNIST neural network using the framework.
- **Project:** Build and train a neural network with more layers and better accuracy on MNIST.

---

### **Weeks 3–4: Improving Deep Neural Networks**  
(Corresponds to **Course 2: Improving Deep Neural Networks**)

#### Days 15–21: Optimization and Regularization Techniques
- **Learn:** Techniques like batch normalization, dropout, and learning rate decay.
- **Practice:** Apply regularization and optimization techniques to your neural networks.
- **Project:** Revisit your previous projects and improve performance by tuning hyperparameters and applying these techniques.

#### Days 22–28: Hyperparameter Tuning and Model Optimization
- **Learn:** Hyperparameter tuning methods (grid search, random search).
- **Practice:** Use libraries like `Keras Tuner` or `Optuna` to optimize a neural network.
- **Project:** Optimize your MNIST or CIFAR-10 model using tuning techniques for better performance.

---

### **Weeks 5–6: Structuring Machine Learning Projects**  
(Corresponds to **Course 3: Structuring Machine Learning Projects**)

#### Days 29–35: Workflow of a Machine Learning Project
- **Learn:** Best practices in structuring ML projects.
- **Practice:** Organize datasets, split data effectively (train/validation/test).
- **Project:** Structure an end-to-end project, from data collection to model evaluation.

#### Days 36–42: Debugging and Error Analysis
- **Learn:** Techniques for error analysis and debugging in deep learning.
- **Practice:** Apply error analysis to improve your models.
- **Project:** Perform detailed error analysis on your projects and make iterative improvements.

---

### **Weeks 7–8: Convolutional Neural Networks (CNNs)**  
(Corresponds to **Course 4: Convolutional Neural Networks**)

#### Days 43–49: CNNs for Image Classification
- **Learn:** Convolutions, pooling, and CNN architectures.
- **Practice:** Build a basic CNN for image classification.
- **Project:** Train a CNN on the CIFAR-10 dataset and analyze its performance.

#### Days 50–56: Transfer Learning
- **Learn:** Use pre-trained models (ResNet, VGG16) for transfer learning.
- **Practice:** Fine-tune a pre-trained CNN for a custom dataset.
- **Project:** Apply transfer learning to classify new image categories (e.g., dog breeds or medical images).

---

### **Weeks 9–10: Recurrent Neural Networks (RNNs) and Sequence Models**  
(Corresponds to **Course 5: Sequence Models**)

#### Days 57–63: RNNs and LSTMs
- **Learn:** RNNs, LSTMs, and their applications for sequential data (e.g., text).
- **Practice:** Implement an RNN or LSTM for text generation.
- **Project:** Train an RNN/LSTM for sentiment analysis on the IMDb dataset.

#### Days 64–70: Word Embeddings and Transformers
- **Learn:** Word embeddings (Word2Vec, GloVe) and attention mechanisms.
- **Practice:** Implement a text classifier using word embeddings.
- **Project:** Fine-tune a transformer model (like BERT) for a task like question answering or text classification.

---

### **Weeks 11–12: Advanced Topics in Deep Learning**  
(Standalone weeks, complementing but not covered directly by Coursera)

#### Days 71–77: Generative Adversarial Networks (GANs)
- **Learn:** GANs and their architecture.
- **Practice:** Build a simple GAN to generate images (e.g., MNIST or fashion images).
- **Project:** Train a GAN on a custom dataset, like generating artwork or fashion items.

#### Days 78–84: Autoencoders and Dimensionality Reduction
- **Learn:** Autoencoders for data compression and anomaly detection.
- **Practice:** Build an autoencoder for image reconstruction.
- **Project:** Use autoencoders for anomaly detection on a dataset like CIFAR-10.

---

### **Weeks 13–14: Capstone Project**

#### Days 85–100: End-to-End Deep Learning Project
- **Project:** Choose a comprehensive project that brings together what you've learned.
  - **Data Collection and Preparation:** Gather and preprocess a dataset.
  - **Model Building:** Choose the best architecture (CNN, RNN, transformer).
  - **Training and Optimization:** Train your model and apply hyperparameter tuning and regularization.
  - **Deployment:** Deploy the model using Flask, Django, or TensorFlow Serving.

---

### Recap:
- **Weeks 1–2**: Basics of neural networks and deep learning.
- **Weeks 3–4**: Optimizing and improving neural networks.
- **Weeks 5–6**: Structuring ML projects and error analysis.
- **Weeks 7–8**: CNNs and image classification.
- **Weeks 9–10**: RNNs, sequence models, and NLP tasks.
- **Weeks 11–12**: Advanced topics like GANs and autoencoders.
- **Weeks 13–14**: Capstone project, combining all skills.

This version of the plan follows the structure of Andrew Ng's specialization while ensuring you cover all the core topics in your original plan.