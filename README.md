# DL 100 challenge
 2h/d DeepLearning plan.

### Weeks 1–2: Foundations of Deep Learning

#### Days 1–7: Neural Networks Basics  
- Day 1–3 (Learn): Perceptrons, activation functions  
  - Detail: Study how activation functions introduce non-linearity into the network.  
- Day 4–5 (Practice): Build a simple neural network from scratch using Python/NumPy.  
  - Detail: Focus on implementing forward propagation and backpropagation.  
- Day 6–7 (Project): Train a small neural network to recognize MNIST handwritten digits.  
  - Detail: Work on hyperparameter tuning and evaluating your model performance.

#### Days 8–14: Deep Learning with TensorFlow/PyTorch  
- Day 8–9 (Learn): Introduction to TensorFlow or PyTorch.  
  - Detail: Explore the basic building blocks (Tensors, Models, Optimizers).  
- Day 10–11 (Practice): Re-implement your MNIST neural network using TensorFlow or PyTorch.  
  - Detail: Add more layers to your model for a deeper network.  
- Day 12–14 (Project): Improve accuracy and visualize training progress with tensorboard or custom functions.

---

### Weeks 3–4: Deep Learning Architectures

#### Days 15–21: CNNs  
- Day 15–16 (Learn): CNNs, convolution, and pooling layers.  
  - Detail: Learn about feature extraction with convolutional filters.  
- Day 17–19 (Practice): Implement a basic CNN.  
  - Detail: Train on MNIST and compare with your previous neural network.  
- Day 20–21 (Project): Train your CNN on CIFAR-10 for a more complex dataset.

#### Days 22–28: RNNs and LSTMs  
- Day 22–23 (Learn): RNNs, LSTMs, and sequential data.  
  - Detail: Understand how hidden states capture dependencies in sequences.  
- Day 24–26 (Practice): Build a simple RNN for text prediction.  
  - Detail: Test on simple text data, like Shakespeare or weather patterns.  
- Day 27–28 (Project): Train an LSTM model on IMDb sentiment analysis dataset.

---

### Weeks 5–6: Deep Learning for Computer Vision

#### Days 29–35: Transfer Learning  
- Day 29–30 (Learn): Understand transfer learning and pre-trained models.  
  - Detail: Study architectures like VGG16 or ResNet.  
- Day 31–33 (Practice): Use a pre-trained model for classification.  
  - Detail: Work on fine-tuning and using custom datasets.  
- Day 34–35 (Project): Apply transfer learning to a custom image dataset, like dog breeds or landmarks.

#### Days 36–42: Data Augmentation and Regularization  
- Day 36–37 (Learn): Learn dropout, L2 regularization, and data augmentation.  
  - Detail: Understand how these techniques prevent overfitting.  
- Day 38–40 (Practice): Implement these techniques in your existing models.  
  - Detail: Apply augmentations like rotations, flips, and shifts to the training images.  
- Day 41–42 (Project): Fine-tune a CNN model with augmentation and dropout for better generalization.

---

### Weeks 7–8: NLP with Deep Learning

#### Days 43–49: Word Embeddings and Text Classification  
- Day 43–45 (Learn): Learn about word embeddings (Word2Vec, GloVe).  
  - Detail: Explore how embeddings capture the meaning of words in vector space.  
- Day 46–47 (Practice): Train a text classifier using embeddings.  
  - Detail: Use embeddings as input to an LSTM or simple RNN.  
- Day 48–49 (Project): Build a text classifier for classifying news articles.

#### Days 50–56: Transformers and Attention Mechanisms  
- Day 50–51 (Learn): Learn about transformers and attention mechanisms.  
  - Detail: Study self-attention and its impact on NLP tasks.  
- Day 52–54 (Practice): Implement a simple transformer model.  
  - Detail: Use libraries like Hugging Face Transformers.  
- Day 55–56 (Project): Fine-tune a pre-trained transformer model like BERT on a custom NLP task (e.g., question answering).

---

### Weeks 9–10: Advanced Topics in Deep Learning

#### Days 57–63: GANs  
- Day 57–58 (Learn): Learn the architecture of GANs.  
  - Detail: Understand how generators and discriminators work together.  
- Day 59–60 (Practice): Build a simple GAN to generate images (e.g., MNIST).  
  - Detail: Focus on loss functions and how they impact learning.  
- Day 61–63 (Project): Train a GAN on a custom dataset like fashion items or artwork.

#### Days 64–70: Autoencoders and Dimensionality Reduction  
- Day 64–65 (Learn): Learn about autoencoders and dimensionality reduction.  
  - Detail: Study how bottlenecks capture the essence of data.  
- Day 66–67 (Practice): Build an autoencoder for image reconstruction.  
  - Detail: Apply autoencoders to noisy data for denoising.  
- Day 68–70 (Project): Use an autoencoder for anomaly detection on datasets like CIFAR-10.

---

### Weeks 11–12: Specialization and Optimization

#### Days 71–77: Hyperparameter Tuning and Model Optimization  
- Day 71–73 (Learn): Learn techniques like grid search and random search for hyperparameter tuning.  
  - Detail: Study libraries like Keras Tuner or Optuna for automating tuning.  
- Day 74–75 (Practice): Apply hyperparameter tuning to a previous model.  
  - Detail: Focus on optimizing learning rate, batch size, and layer sizes.  
- Day 76–77 (Project): Optimize a deep learning model and improve its accuracy.

#### Days 78–84: Model Deployment  
- Day 78–80 (Learn): Learn about model deployment using Flask/Django or TensorFlow Serving.  
  - Detail: Study how to set up an API endpoint for inference.  
- Day 81–82 (Practice): Deploy a model as a web service using Flask.  
  - Detail: Use a trained model to classify user-uploaded images.  
- Day 83–84 (Project): Build a web app with image or text classification capabilities.

---

### Weeks 13–14: Capstone Project

#### Days 85–100 (Capstone Project)  
- Day 85–87: Choose a project, gather data, and preprocess the dataset.  
- Day 88–91: Build your model architecture (CNN, RNN, transformer, etc.).  
- Day 92–95: Train the model, fine-tune hyperparameters, and optimize.  
- Day 96–98: Deploy the model as a web app or mobile app.  
- Day 99–100: Test the app and document the project.

At the end, you’ll have practical knowledge and a strong portfolio!

